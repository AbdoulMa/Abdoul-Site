[
  {
    "path": "posts/2021-09-03-club-predictions/",
    "title": "Club Predictions",
    "description": "Scraping of european clubs predictions and visualization of championship winning probabilities.",
    "author": [
      {
        "name": "Abdoul ISSA BIDA",
        "url": {}
      }
    ],
    "date": "2021-09-03",
    "categories": [
      "R",
      "Tidyverse",
      "Soccer",
      "Scraping"
    ],
    "contents": "\nHi everyone and welcome in my second blog post.\nFor this one, we will cover together two of my favorite disciplines, one in Computer Science, Scraping and the other one in real life, Soccer .\nDon‚Äôt be disappointed üòÑ, if you are there only for the final dataviz, you can skip to the next section. I tried to make it as much as clear and simple that I can.\nScraping\nSo, what website we are going to scrape ?\nIt will be FiveThirtyEight. They provide data behind some of their articles and charts, including data for Soccer Clubs Predictions.\nUnfortunately, the data you can retrieve only cover Club Soccer Predictions and Global Club Soccer Rankings. But our today tutorial data, is based on determining which league club will qualify for UCL1 or which will win the national league.\nSo, we will scrape it directly, from the league page. For example, to scrape, the probabilities for each club of:\nFrench Ligue 1, we will scrape it from https://projects.fivethirtyeight.com/soccer-predictions/ligue-1/\nGerman Bundesliga, we will scrape it from https://projects.fivethirtyeight.com/soccer-predictions/bundesliga/\nEnglish Premier League, we will scrape it from https://projects.fivethirtyeight.com/soccer-predictions/premier-league/\nand so on.\nHere is an example of how, the data is presented on their website.\n\n\n\n\nThe data is updated daily, what is very interesting because, with some tricky automation, we can follow the evolution of the odds of the clubs to win a season along. But, that is not the subject of this blog post.\nWebsite Page Reading\nFor the scraping, we need a couple libraries, in particular:\n\n\nlibrary(tidyverse) # For data wrangling, ggplot2 and friends\nlibrary(rvest) # for the scraping \nlibrary(janitor) # for the function row_to_names\n\n\n\nSo, let‚Äôs start our scraping workflow :\n\n\nleague_link <- \"https://projects.fivethirtyeight.com/soccer-predictions/premier-league/\"\nclubs_rows <- league_link %>% \n  read_html() %>% # Retrieve the complete table \n  html_element(\"#forecast-table\") %>%  # Retrieve only the forecast table\n  html_elements(\"tbody .team-row\") # Retrieve each row of the table\n\n\n\nLet‚Äôs me explain a little bit the code.\nFirstly, I retrieve the complete page.\n\n\nleague_link %>% \n  read_html()\n\n\n\nSecondly, I retrieve the forecasting table, with the function html_element(). So, where #forecast-table comes from?\nTo be a good web scraper, you must be a good website inspector. Web developers, create websites with logic, and in order to retrieve data from those website pages, we have to make to make us their logic.\nTo find out how to access the forecast table, you must go to the page we are scraping (here). Right-click on the table we want to retrieve, and then click inspect. The browser will open the inspector.\n\n\n\nFigure 1: Inspector Interface\n\n\n\nI use Mozilla Firefox as my browser, you will probably need another process depending on your browser. Google it, if you don‚Äôt know how to do it.\nNext, you need a little attention to notice that the table has as id forecast-table. It also has as class forecast-table. But, we will use the id to access the table.\nFor this, we use the html_element() function of the rvest(Wickham 2021) package. When we select the table by its id, we prefix the id with # in our html_element() function.\nIn the same way, we collect each club row with:\n\n\n... %>% \nhtml_elements(\"tbody .team-row\")\n\n\n\nNote that we are using, html_elements() instead of html_element(), which selects all the elements (and not just the first one) of our forecast table.\nLet‚Äôs see what the list of results looks like.\n\n\nclubs_rows\n\n\n{xml_nodeset (20)}\n [1] <tr class=\"team-row\" data-str=\"Manchester City\">\\n<td class=\"t ...\n [2] <tr class=\"team-row\" data-str=\"Liverpool\">\\n<td class=\"team\" d ...\n [3] <tr class=\"team-row\" data-str=\"Chelsea\">\\n<td class=\"team\" dat ...\n [4] <tr class=\"team-row\" data-str=\"Manchester United\">\\n<td class= ...\n [5] <tr class=\"team-row\" data-str=\"Tottenham Hotspur\">\\n<td class= ...\n [6] <tr class=\"team-row\" data-str=\"Everton\">\\n<td class=\"team\" dat ...\n [7] <tr class=\"team-row\" data-str=\"West Ham United\">\\n<td class=\"t ...\n [8] <tr class=\"team-row\" data-str=\"Brighton and Hove Albion\">\\n<td ...\n [9] <tr class=\"team-row\" data-str=\"Leicester City\">\\n<td class=\"te ...\n[10] <tr class=\"team-row\" data-str=\"Arsenal\">\\n<td class=\"team\" dat ...\n[11] <tr class=\"team-row\" data-str=\"Wolverhampton\">\\n<td class=\"tea ...\n[12] <tr class=\"team-row\" data-str=\"Aston Villa\">\\n<td class=\"team\" ...\n[13] <tr class=\"team-row\" data-str=\"Leeds United\">\\n<td class=\"team ...\n[14] <tr class=\"team-row\" data-str=\"Crystal Palace\">\\n<td class=\"te ...\n[15] <tr class=\"team-row\" data-str=\"Southampton\">\\n<td class=\"team\" ...\n[16] <tr class=\"team-row\" data-str=\"Brentford\">\\n<td class=\"team\" d ...\n[17] <tr class=\"team-row\" data-str=\"Burnley\">\\n<td class=\"team\" dat ...\n[18] <tr class=\"team-row\" data-str=\"Newcastle\">\\n<td class=\"team\" d ...\n[19] <tr class=\"team-row\" data-str=\"Watford\">\\n<td class=\"team\" dat ...\n[20] <tr class=\"team-row\" data-str=\"Norwich City\">\\n<td class=\"team ...\n\nWell, we have all, the premier league clubs.\nClubs names and logos\nThe next step in my workflow is to select for each club, its name and logo link. You should be wondering, why I am not selecting the probabilities I was talking at the beginning. Please be patient, this will be the subject of our next section.\nLet‚Äôs get the name and the logo for one club, and then generalize for all.\n\n\n# Let's select the first node \nnode <- pluck(clubs_rows, 1)\n team_name <- node %>% \n    html_element(\".team-div .name\") %>% # Select Team name elmt\n    html_text2() %>% # Retrieve the text\n    # Delete the points in the name\n    # Example: Man City8pts becomes Man City\n    str_remove(pattern =\"\\\\d+\\\\spts?\") \n  \n  team_logo <- node %>% \n    # Select Team the img which contains team logo\n    html_element(\".logo img\") %>% \n    # Retrieve the the src attribute\n    html_attr(\"src\") %>% \n    str_remove(\"&w=56\")\n\n\n\nLet‚Äôs see if everything is what it supposed to.\n\n\nprint(team_name)\n\n\n[1] \"Man. City\"\n\n\n\nprint(team_logo)\n\n\n[1] \"https://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/382.png\"\n\nIt is perfect, we can retrieve from a node, the club name and its logo. Let us generalize to all the clubs with a function.\n\n\nextract_name_logo <- function(node) { \n  team_name <- node %>% \n    html_element(\".team-div .name\") %>% # Select Team name element\n    html_text2() %>% # Retrieve the text\n    # Delete the points in the name\n    # Example: \"Man City8pts\" becomes \"Man City\"\n    str_remove(pattern =\"\\\\d+\\\\spts?\") \n  \n  team_logo <- node %>% \n    # Select the img element which contains team logo\n    html_element(\".logo img\") %>% \n    # Retrieve the src attribute\n    html_attr(\"src\") %>% \n    str_remove(\"&w=56\")\n  # Return it like a tibble\n  tibble(\n    team_name,\n    team_logo\n  ) \n}\n\n\n\nThanks to the purrr library, we can now retrieve all clubs names and logos.\n\n\nclubs_names_logos <- clubs_rows %>% \n   map_df(extract_name_logo)\n\n\n\n\n\nTable 1: Team names and logos link\n\n\nteam_name\n\n\nteam_logo\n\n\nMan. City\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/382.png\n\n\nLiverpool\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/364.png\n\n\nChelsea\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/363.png\n\n\nMan. United\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/360.png\n\n\nTottenham\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/367.png\n\n\nEverton\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/368.png\n\n\nWest Ham\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/371.png\n\n\nBrighton\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/331.png\n\n\nLeicester\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/375.png\n\n\nArsenal\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/359.png\n\n\nWolves\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/380.png\n\n\nAston Villa\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/362.png\n\n\nLeeds United\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/357.png\n\n\nCrystal Palace\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/384.png\n\n\nSouthampton\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/376.png\n\n\nBrentford\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/337.png\n\n\nBurnley\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/379.png\n\n\nNewcastle\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/361.png\n\n\nWatford\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/395.png\n\n\nNorwich\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/381.png\n\n\nRetrieve the forecast table\nIn this section, we will use another function from rvest package : html_table(). This function mimics what what a browser does, but repeats the values of merged cells in every cell that cover.\n\n\nclubs_predictions <- league_link %>% \n  read_html() %>% \n  html_element(\"#forecast-table\") %>% \n  # Don't keep the header \n  html_table(header = F) %>% \n  # Remove extra headers that we don't need\n  # And make the third row the columns names \n  janitor::row_to_names(row_number = 3) %>%\n  # Remove extra columns that we don't need\n  select(1:10) %>% \n  mutate(\n  # Delete the points in the name\n    # Example: Man City8pts becomes Man City\n    team_name = str_remove(team ,pattern =\"\\\\d+\\\\spts?\") \n   ) %>% \n  relocate(team_name) %>% \n    select(-team)\n\n\n\nI know it can be a little bit complex for a beginner (6 months ago I was too). But nothing exceptional, if you understand the logic behind each function.\nWhat the data looks like at this stage?\n\n\nTable 2: Clubs Predictions\n\n\nteam_name\n\n\nspi\n\n\noff.\n\n\ndef.\n\n\ngoal diff.\n\n\nproj. pts.pts.\n\n\nEvery position\n\n\nrelegatedrel.\n\n\nqualify for UCLmake UCL\n\n\nwin Premier Leaguewin league\n\n\nMan. City\n\n\n93.5\n\n\n2.9\n\n\n0.2\n\n\n+58\n\n\n85\n\n\n\n\n<1%\n\n\n93%\n\n\n47%\n\n\nLiverpool\n\n\n90.0\n\n\n2.7\n\n\n0.4\n\n\n+45\n\n\n79\n\n\n\n\n<1%\n\n\n83%\n\n\n25%\n\n\nChelsea\n\n\n88.5\n\n\n2.4\n\n\n0.3\n\n\n+37\n\n\n76\n\n\n\n\n<1%\n\n\n74%\n\n\n16%\n\n\nMan. United\n\n\n85.6\n\n\n2.4\n\n\n0.5\n\n\n+30\n\n\n71\n\n\n\n\n<1%\n\n\n59%\n\n\n9%\n\n\nTottenham\n\n\n76.1\n\n\n2.2\n\n\n0.7\n\n\n+4\n\n\n59\n\n\n\n\n2%\n\n\n19%\n\n\n1%\n\n\nEverton\n\n\n75.0\n\n\n2.0\n\n\n0.7\n\n\n+4\n\n\n56\n\n\n\n\n3%\n\n\n14%\n\n\n<1%\n\n\nWest Ham\n\n\n74.9\n\n\n2.1\n\n\n0.8\n\n\n+5\n\n\n55\n\n\n\n\n3%\n\n\n13%\n\n\n<1%\n\n\nBrighton\n\n\n74.2\n\n\n1.8\n\n\n0.6\n\n\n-2\n\n\n53\n\n\n\n\n4%\n\n\n9%\n\n\n<1%\n\n\nLeicester\n\n\n74.4\n\n\n2.0\n\n\n0.7\n\n\n-1\n\n\n53\n\n\n\n\n5%\n\n\n10%\n\n\n<1%\n\n\nArsenal\n\n\n76.3\n\n\n2.0\n\n\n0.7\n\n\n-4\n\n\n52\n\n\n\n\n6%\n\n\n8%\n\n\n<1%\n\n\nWolves\n\n\n74.0\n\n\n1.8\n\n\n0.6\n\n\n-3\n\n\n48\n\n\n\n\n9%\n\n\n5%\n\n\n<1%\n\n\nAston Villa\n\n\n72.2\n\n\n2.0\n\n\n0.8\n\n\n-7\n\n\n48\n\n\n\n\n10%\n\n\n4%\n\n\n<1%\n\n\nLeeds United\n\n\n69.3\n\n\n2.0\n\n\n0.9\n\n\n-14\n\n\n45\n\n\n\n\n16%\n\n\n2%\n\n\n<1%\n\n\nCrystal Palace\n\n\n66.0\n\n\n1.7\n\n\n0.9\n\n\n-14\n\n\n43\n\n\n\n\n20%\n\n\n1%\n\n\n<1%\n\n\nSouthampton\n\n\n66.8\n\n\n1.8\n\n\n0.9\n\n\n-15\n\n\n42\n\n\n\n\n23%\n\n\n1%\n\n\n<1%\n\n\nBrentford\n\n\n65.5\n\n\n1.6\n\n\n0.8\n\n\n-15\n\n\n41\n\n\n\n\n24%\n\n\n1%\n\n\n<1%\n\n\nBurnley\n\n\n63.6\n\n\n1.8\n\n\n1.0\n\n\n-21\n\n\n38\n\n\n\n\n34%\n\n\n<1%\n\n\n<1%\n\n\nNewcastle\n\n\n62.2\n\n\n1.8\n\n\n1.1\n\n\n-26\n\n\n37\n\n\n\n\n38%\n\n\n<1%\n\n\n<1%\n\n\nWatford\n\n\n59.0\n\n\n1.6\n\n\n1.0\n\n\n-29\n\n\n34\n\n\n\n\n49%\n\n\n<1%\n\n\n<1%\n\n\nNorwich\n\n\n59.4\n\n\n1.6\n\n\n1.0\n\n\n-32\n\n\n32\n\n\n\n\n54%\n\n\n<1%\n\n\n<1%\n\n\nLet‚Äôs clean the data a bit more to make it fit what we want to do.\n\n\nclubs_predictions <- clubs_predictions %>%\n  # the column with \"win league\" has different\n  # name according to the league so I rename it\n  # to \"win_league\" for all leagues\n  mutate(across(contains(\"win league\"), ~ ., .names = \"win_league\")) %>%\n  # Rename important columns\n  rename(goal_diff = \"goal diff.\",\n         proj_pts = \"proj. pts.pts.\",\n         qualify_ucl = \"qualify for UCLmake UCL\"\n  ) %>%\n  # Delete columns with space in their names \n  select(-contains(\" \"))\n\n# When probability <1%, give  it 0\nclubs_predictions <- clubs_predictions %>%\n  mutate(across(.cols = c(\"relegatedrel.\", \"qualify_ucl\", \"win_league\"), .fns = ~ if_else(. == \"<1%\", \"0\", .))) %>%\n  mutate(across(.cols = c(\"relegatedrel.\", \"qualify_ucl\", \"win_league\"), .fns = ~ parse_number(.)))\n\n\n\nFinally, let‚Äôs join the clubs predictions dataframe with names and logos dataframe previously scraped.\n\n\nclubs_predictions <- clubs_predictions %>%\n  left_join(clubs_names_logos)\n\n\n\n\n\n\nTable 3: Clubs Predictions and Teams Informations\n\n\nteam_name\n\n\nspi\n\n\noff.\n\n\ndef.\n\n\ngoal_diff\n\n\nproj_pts\n\n\nrelegatedrel.\n\n\nqualify_ucl\n\n\nwin_league\n\n\nteam_logo\n\n\nMan. City\n\n\n93.5\n\n\n2.9\n\n\n0.2\n\n\n+58\n\n\n85\n\n\n0\n\n\n93\n\n\n47\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/382.png\n\n\nLiverpool\n\n\n90.0\n\n\n2.7\n\n\n0.4\n\n\n+45\n\n\n79\n\n\n0\n\n\n83\n\n\n25\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/364.png\n\n\nChelsea\n\n\n88.5\n\n\n2.4\n\n\n0.3\n\n\n+37\n\n\n76\n\n\n0\n\n\n74\n\n\n16\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/363.png\n\n\nMan. United\n\n\n85.6\n\n\n2.4\n\n\n0.5\n\n\n+30\n\n\n71\n\n\n0\n\n\n59\n\n\n9\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/360.png\n\n\nTottenham\n\n\n76.1\n\n\n2.2\n\n\n0.7\n\n\n+4\n\n\n59\n\n\n2\n\n\n19\n\n\n1\n\n\nhttps://secure.espn.com/combiner/i?img=/i/teamlogos/soccer/500/367.png\n\n\n\nData Visualization\nWell, we had our data, tidy as we wanted. Now let‚Äôs visualize it. If you skip the scraping workflow, you can download the data for this section Here.\nWe are going to visualize it as a facet of a waffle plot for each team. Since the probabilities are represented as percentage, we are going to make a waffle of 100 squares. Each represents a chance for a club to win the league, to qualify for UEFA Champions League or both.\nHowever, to fill the square according to each category of probability, it is necessary to wrangle the data a little bit more, in particular to bring together in a single column the three categories we want to highlight.\nSo what do I do?\n\n\npredictions_waffle_df <- clubs_predictions %>% \n  mutate(ucl_qualif_diff = qualify_ucl - win_league,\n         remaining = 100 - qualify_ucl) %>% \n  pivot_longer(\n    cols = c(\"win_league\", \"ucl_qualif_diff\",\"remaining\"), \n    names_to = \"win_cat\", \n    values_to = \"win_value\"\n  )\n\n\n\nFirst, I create two new columns:\nucl_qualif_diff which represents the probability that a club qualifies to UEFA Champions League\nremaining which represents the probability that a club won‚Äôt win the league and won‚Äôt qualify for the UEFA Champions League.\nAnd finally, i am grouping my three categories into a single column win_cat and their values in the win_value column.\nSo let‚Äôs finally make the waffle.\nWe will be using waffle package by Bob Rudis, which is clearly one of my favorites.\nUnfortunately, the package is not available on CRAN, so let‚Äôs install it with devtools:\n\n\ndevtools::install_github(\"hrbrmstr/waffle\")\n\n\n\nWe will need a few more packages to polish our visualization:\n\n\nlibrary(waffle)\nlibrary(ggtext) # For customize the text \nlibrary(ragg) # For the device  to save the plot\n\n\n\nTo draw club logo images, let‚Äôs define a special function:\n\n\n# The function takes 2 parameters \n# x which refers to club logo link we scraped early  \n# width for the img width with default value 30\nlink_to_img <- function(x, width = 30) {\n  # Define the logo link as src attribute to \n  # html img element\n  glue::glue(\"<img src='{x}' width='{width}'/>\")\n}\n\n\n\nFinally let‚Äôs implement our visualization.\n\n\nplot <- predictions_waffle_df %>% \n  mutate( team_name = fct_reorder(paste0(link_to_img(team_logo),'<br>',team_name), -qualify_ucl), \n    win_cat = fct_relevel(win_cat, c(\"win_league\", \"ucl_qualif_diff\",\"remaining\"))) %>% \n  ggplot(aes(fill = win_cat, values = win_value)) + \n  geom_waffle(color = \"white\", size = .15, n_rows = 10, flip = T) + \n  facet_wrap(vars(team_name)) + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\n      \"win_league\" = \"#117733\",\n      \"ucl_qualif_diff\" = alpha(\"#117733\",.5),\n      \"remaining\" = alpha(\"#117733\",.1)\n    ) ,\n    labels = \n      c(\n        \"win_league\" = \"Win League & Qualify to UCL\",\n        \"ucl_qualif_diff\" = \"Qualify to UCL\",\n        \"remaining\" = \"No chance\"\n      )\n  ) +\n  labs(title = \"English Premier League Clubs Predictions\") +\n  coord_equal(expand = F) + \n  theme_minimal(base_family = \"Chivo\") +\n  theme(\n    plot.background = element_rect(fill = \"grey95\", color = NA),\n    panel.border = element_rect(color = \"black\", size = 1.1, fill = NA),\n    legend.position = \"top\",\n    plot.margin = margin( b = 1, unit = \"cm\"),\n    plot.title = element_text(size = rel(2), margin = margin(t = 20, b= 20)),\n        axis.text = element_blank(),\n        strip.text = element_markdown())\n\n\n\n\n\n\n\nEt voil√†!\n\n\n\n\nWickham, Hadley. 2021. Rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest.\n\n\nUEFA Champions League‚Ü©Ô∏é\n",
    "preview": "posts/2021-09-03-club-predictions/preview.png",
    "last_modified": "2021-09-12T20:08:01+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-treemap/",
    "title": "Tired of Pie Charts !? Let's make a Tree Map.",
    "description": "Tree maps are useful alternatives for the presentation of proportions.",
    "author": [
      {
        "name": "Abdoul ISSA BIDA",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [
      "R",
      "Tidyverse",
      "Tree Map"
    ],
    "contents": "\nData Reading\nFor this first blog post, we are going to use Tidytuesday(Mock 2021) Week 34 Dataset.\nThe data comes from SpeechInteraction.org and was shared by Sara Stoudt.\nIt is the records of Star Trek characters interactions with Voice User Interfaces(VUIs) such as Apple Siri, Google Assistant, Amazon Alexa.\nA complete description of the dataset is available at : https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-08-17.\nFirst of all, let‚Äôs import tidyverse package :\n\n\nlibrary(tidyverse) # Import ggplot2 and friends for data wrangling \n\n\n\nSecondly, let‚Äôs read the data :\n\n\ncomputer <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-17/computer.csv')\n\n\n\nHere is what the data looks like.\n\n\nhead(computer, 2)\n\n\n\n\n\n\nname\n\n\nchar\n\n\nline\n\n\ndirection\n\n\ntype\n\n\npri_type\n\n\ndomain\n\n\nsub_domain\n\n\nnv_resp\n\n\ninteraction\n\n\nchar_type\n\n\nis_fed\n\n\nerror\n\n\nvalue_id\n\n\n102\n\n\nTasha\n\n\nBattle bridge.\n\n\nThe doors snap closed and the lift moves. Riker looks Tasha over waiting then:\n\n\nStatement\n\n\nStatement\n\n\nIoT\n\n\nTurbolift\n\n\nTRUE\n\n\nBattle bridge.\n\n\nPerson\n\n\nTRUE\n\n\nFALSE\n\n\n255\n\n\n102\n\n\nBeverly\n\n\nShow me the results of Captain Picard‚Äôs most recent physical examination.\n\n\nThe screen promptly BEGINS TO FLASH UP PRINTED INFORMATION followed by X-Ray type shots etc. Beverly studies it for awhile.\n\n\nCommand\n\n\nCommand\n\n\nInfoSeek\n\n\nNA\n\n\nTRUE\n\n\nShow me the results of Captain Picard‚Äôs most recent physical examination.\n\n\nPerson\n\n\nTRUE\n\n\nFALSE\n\n\n345\n\n\n\nData Wrangling\nWe won‚Äôt spend a lot of time wrangling data to try to bring out some specific pattern. We are going to simply count the characters which interact the more with VUIs.\n\n\ncharacters <- computer %>% \n  count(char, sort= T) %>%   \n  # Categorize  the  characters that will be useful for our visualization\n  mutate(char = factor(char)) \n\n\n\nLet‚Äôs look at, the 6 characters which interact the most with VUIs.\n\n\nhead(characters)\n\n\n\n\n\nchar\n\n\nn\n\n\nComputer Voice\n\n\n598\n\n\nGeordi\n\n\n320\n\n\nPicard\n\n\n266\n\n\nData\n\n\n235\n\n\nRiker\n\n\n150\n\n\nBeverly\n\n\n121\n\n\nThe results are logical because Geordi is an engineer in The Star Trek Saga.\nGraphic\nFor this section, we need to import some extra libraries:\ntreemapify(Wilkins 2021)\npaletteer(file. 2021)\nggtext\nragg\n\n\nlibrary(treemapify) # For geom_treemap and friends\nlibrary(paletteer) # For color palette \nlibrary(ggtext) # For customize text (used in this script with element_markdown)\nlibrary(ragg) # For the device for save the plot\n\n\n\nSo why a Tree Map, instead of a Pie chart for proportions Visualization?\nPie charts are generally really cool to look at. But in some cases, like ours, there is too many individual data, and they are so close in magnitude that the message that we try to spread with a pie chart is biased.\nIn those cases, I use Tree Maps, which are pretty useful for the presentation of proportions.\n\nA tree map shows the attribute of a cardinally scaled variable as nested rectangles. The size and order of the rectangles are calculated so that, with preset outer dimensions, the large rectangle is completely filled and the areas of the individual rectangles correspond to the size of the variables.(Rahlf 2017)\n\n\ncharacters %>%\n  ggplot(aes(fill = char,area = n)) + \n  geom_treemap(color = \"black\", size = 1) + \n  # We won't use legends because we 'ill annotate each square \n  # with the character name and the number of times it interacted \n  theme(legend.position = \"none\") \n\n\n\n\nAt this step, we just mapped, each area with the number of interactions of a character and filled the area with the character.\nNext step, let‚Äôs annotate each area with the name of the matching character using the function geom_treemap_text of treemapify package.\nThis function takes several arguments especially:\nfamily for the font family\nfontface for the font face\ncolour for the font color\nplace for the place inside the box where the text is placed.\ngrow which is is very important, because if TRUE, the text will be grown as well as shrunk to fill the box.\n\n\ncharacters %>%\n  # Add in the mapping, label\n  ggplot(aes(fill = char,area = n,label = glue::glue(\" {char} \\n ({n})\"))) + \n  geom_treemap(color = \"black\", size = 1) + \n  geom_treemap_text(family = \"Lato Black\",fontface = \"italic\",\n                    colour = \"white\", place = \"centre\",\n                    grow = TRUE) + \n  theme(legend.position = \"none\") \n\n\n\n\nIt began to look pretty good, but let‚Äôs customize it a little bit.\nFor that, I will fill the areas with a different color palette. The challenge would be to find a color palette with more 25 colors.\nFortunately for us, colorRampPalette() R function provide a tools to interpolate a set of given colors to create new color palettes.\nC√©dric Scherer published a trick about that.\n\n\n\n\n{\"x\":{\"twid\":\"1426157378454556672\",\"pars\":{\"align\":\"center\"}},\"evals\":[],\"jsHooks\":[]}\n\n\n\nLet‚Äôs use it. For color palette, I choose Prism from rcartoclor package. We can also access this palette within paletteer package.\n\n\npaletteer_d(\"rcartocolor::Prism\",12) %>% \n  # We visualize the colors with show_col from scales pkg \n  scales::show_col()\n\n\n\n\nNow, we extend it.\n\n\nextended_palette <- colorRampPalette(paletteer_d(\"rcartocolor::Prism\",12)) \n# Let's visualize 49 interpolated colors et\nextended_palette(49) %>% \n  scales::show_col(cex_label = .55, ncol = 7)\n\n\n\n\nFinally we can use it, and set a little bit more the theme.\n\n\nplot <- characters %>%\n  ggplot(aes(fill = char,area = n, label = glue::glue(\" {char} \\n ({n})\"))) + \n  geom_treemap(color = \"black\", size = 1) + \n  geom_treemap_text(family = \"Lato Black\",fontface = \"italic\", colour = \"white\", place = \"centre\",\n                    grow = TRUE) + \n  labs(title = \"Which characters interact the most with VUIs\\n in the Star Trek Saga?\",\n       caption = \"Data from ***SpeechInteraction.com*** and shared by Sara Stoudt.<br/>\n       Tidytuesday Week-34 2021.\") + \n  scale_fill_manual(values = extended_palette(nrow(characters))) + \n  theme(text =element_text(family = \"Lato\"),\n        plot.background = element_rect(fill = \"grey95\"),\n        panel.spacing = unit(2.5, units = \"cm\"),\n        plot.title = element_text(family = \"Lato Black\",size = rel(2.5), hjust = .5, margin = margin(t = 15,b = 10)),\n        plot.caption = element_markdown(color = \"black\", size = rel(1.2), margin = margin(t = 20,b = 10)),\n        legend.position = \"none\"\n  )\n\n\n\n\n\n\n\nEt voil√†!\n\n\n\n\nfile., See AUTHORS. 2021. Paletteer: Comprehensive Collection of Color Palettes. https://CRAN.R-project.org/package=paletteer.\n\n\nMock, Thomas. 2021. ‚ÄúTidy Tuesday: A Weekly Data Project Aimed at the r Ecosystem.‚Äù https://github.com/rfordatascience/tidytuesday.\n\n\nRahlf, Thomas. 2017. Data Visualisation with r 100 Examples.\n\n\nWilkins, David. 2021. Treemapify: Draw Treemaps in ‚ÄôGgplot2‚Äô. https://CRAN.R-project.org/package=treemapify.\n\n\n\n\n",
    "preview": "posts/2021-08-17-treemap/preview.png",
    "last_modified": "2021-09-09T21:49:51+02:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": {},
    "author": [
      {
        "name": "Abdoul ISSA BIDA",
        "url": "https://abdoulblog.netlify.app"
      }
    ],
    "date": "2021-06-18",
    "categories": [
      "R",
      "Python"
    ],
    "contents": "\nI started learning R a couple months ago, because I spend a lot time on Twitter and was always amazed by all the great graphics made with this statistical programming language. First of all, I followed essentially some great sport Data Analysts like Owen Phillips who post great charts about NBA Players & Community Stats.\n\n\nSome Owen‚Äôs Tweets\n\n\n\n\n{\"x\":{\"twid\":\"1377974728233734148\",\"pars\":{\"align\":\"center\"}},\"evals\":[],\"jsHooks\":[]}\n\n\n\n\n\n{\"x\":{\"twid\":\"1405137683467456513\",\"pars\":{\"align\":\"center\"}},\"evals\":[],\"jsHooks\":[]}\n\n\n‚Äì Owen Pihllips(@owenlhjphillips)\n\n\nAnd as things progress, I started following more and more people from R Community and particularly those who contribute to the Tidytuesday weekly Data Visualization Challenge.\nAs, my passion keeps growing, I started learning R typically with short tutorial like those amazing ones from C√©dric Scherer or Thomas Mock but also with some bookdown particularly the bible for the beginners and the aspiring data scientists R for Data Science (Wickham and Grolemund 2017).\nOn May 5, 2020, I made my first contribution for the Tidytuesday Challenge. I wasn‚Äôt expecting a lot of likes but what happened was just magical.\n\nThe dataset was about Water Acces Points and come from WPDX. My analysis of the data wasn‚Äôt the adequate one and my lines plot was even less, but my intention was principally to give birth to a presentable plot with my beginner knowledge.\n\n\n{\"x\":{\"twid\":\"1389976276765716488\",\"pars\":{\"align\":\"center\"}},\"evals\":[],\"jsHooks\":[]}\n‚Äì Abdoul Madjid(@issa_madjid)\n\n I received a ton of compliments and advices for that meme lines plot, what acted like a boost for my little ego üòÖ. So from that initial post, I have decided, despite my professional life schedule:\nto be a weekly contributor to the TidyTuesday Challenge\nto try to progress from the feedback I received from the Community\nand to build a blog with {distill} which is used to build scientific and technical writing and native blog with R Markdown\n\nSo Here, We go.\n\nAcknowledgement\nA big thank to all the amazing contributors to the R community.\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; O‚ÄôReilly Media. http://r4ds.had.co.nz/.\n\n\n\n\n",
    "preview": "posts/welcome/preview.png",
    "last_modified": "2021-09-12T20:07:26+02:00",
    "input_file": {}
  }
]
